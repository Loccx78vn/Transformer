[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cao Xu√¢n L·ªôc",
    "section": "",
    "text": "Xin ch√†o, m√¨nh l√† L·ªôc, sinh nƒÉm 2003 v√† l√† m·ªôt ch√†ng trai ƒë·∫øn t·ª´ m·∫£nh ƒë·∫•t ƒë·∫ßy n·∫Øng v√† gi√≥ - Ph√∫ Y√™n, Vi·ªát Nam. M√¨nh c√≥ b·∫±ng c·ª≠ nh√¢n tr∆∞·ªùng ƒê·∫°i h·ªçc Kinh T·∫ø - T√†i Ch√≠nh (UEF) v√† chuy√™n ng√†nh c·ªßa m√¨nh l√† Logistics v√† qu·∫£n l√Ω chu·ªói cung ·ª©ng.\nL√† ng∆∞·ªùi c√≥ ni·ªÅm ƒëam m√™ m·∫°nh m·∫Ω v·ªõi R, m√¨nh c√≥ s·ªü th√≠ch vi·∫øt post v·ªÅ vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi R ƒë·ªÉ ·ª©ng d·ª•ng v√†o c√°c c√¥ng vi·ªác, b√†i to√°n th∆∞·ªùng g·∫∑p trong Supply Chain. Ngo√†i ra, s·ªü th√≠ch c·ªßa m√¨nh l√† nghe s√°ch n√≥i v√† ƒëi b·ªô!\nC√¢u slogan m√† m√¨nh th√≠ch nh·∫•t l√†: ‚ÄúDon‚Äôt fear the risk, fear the opportunity lost!‚Äù v√† ƒë√≥ c≈©ng l√† c√°ch m√¨nh s·ªëng v√† l√†m vi·ªác ƒë·∫øn b√¢y gi·ªù üíùüíùüíù.\nHi v·ªçng c√°c b·∫°n s·∫Ω th√≠ch b√†i vi·∫øt c·ªßa m√¨nh!\n    \n    \n    Go to Next Page\n    \n    \n        \n            Go to Next Page\n            ‚ûî"
  },
  {
    "objectID": "practice.html",
    "href": "practice.html",
    "title": "Time series forecasting",
    "section": "",
    "text": "Khi b·∫°n c·∫ßn x√¢y d·ª±ng c√°c m√¥ h√¨nh Deep learning ph·ª©c t·∫°p h∆°n th√¨ package torch trong R gi·ªëng v·ªõi PyTorch trong Python th∆∞·ªùng d√πng ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh machine learning v√† n√≥ s·∫Ω l√† c√¥ng c·ª• m·∫°nh m·∫Ω c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n (N·∫øu b·∫°n ch∆∞a bi·∫øt th√¨ h·∫ßu h·∫øt packages ƒë·ªÉ train machine learning model trong R ƒë·ªÅu th·ª±c hi·ªán th√¥ng qua Python v√† ƒë∆∞·ª£c b·∫Øt c·∫ßu n·ªëi b·∫±ng package reticulate).\nƒê·ªÉ h·ªçc h·∫øt v·ªÅ torch th√¨ b·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c link sau:\n\nS√°ch Deep Learning and Scientific Computing with R torch c·ªßa Sigrid Keydana.\nM·ªôt lo·∫°t b√†i post t·ª´ posit blog.\n\nC√≤n ·ªü b√†i vi·∫øt n√†y, m√¨nh ch·ªâ gi·ªõi thi·ªáu c∆° b·∫£n c√°ch s·ª≠ d·ª•ng torch trong R ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh.\n\n\n\n\n\n\nTip\n\n\n\nƒê·ªÉ t·∫£i torch v√†o m√°y local th√¨ b·∫°n s·ª≠ d·ª•ng c√∫ ph√°p install.package(\"torch\")\n\n\n\n\n·ªû ƒë√¢y, m√¨nh s·∫Ω gi·ªõi thi·ªáu c∆° b·∫£n ƒë·ªÉ m·ªçi ng∆∞·ªùi c√≥ ki·∫øn th·ª©c c∆° b·∫£n nh·∫•t v·ªÅ torch\nƒê·∫ßu ti√™n, ƒë·ªÉ d√πng package torch trong R th√¨ ta c·∫ßn chuy·ªÉn ƒë·ªïi object sang class tensor th√¨ d√πng h√†m torch_tensor(). ƒê√¢y l√† v√≠ d·ª• v·ªÅ:\n\n\nCode\nlibrary(torch)\nm&lt;-torch_tensor(array(1:24, dim = c(4, 3, 2)))\nclass(m)\n\n\n[1] \"torch_tensor\" \"R7\"          \n\n\nNgo√†i ra, trong object tensor c√≤n ch·ª©a th√™m th√¥ng tin kh√°c nh∆∞ l√†: $dtype s·∫Ω return data type (v√≠ d·ª• nh∆∞ object d∆∞·ªõi ƒë√¢y l√† d·∫°ng long integer), $device return n∆°i tensor object ƒë∆∞·ª£c l∆∞u tr·ªØ, $shape return dimensions c·ªßa object.\n\n\nCode\nm$dtype\n\n\ntorch_Long\n\n\nCode\nm$device\n\n\ntorch_device(type='cpu') \n\n\nCode\nm$shape\n\n\n[1] 4 3 2\n\n\nV√≠ d·ª• ta c√≥ th·ªÉ simulate c√¥ng th·ª©c ƒë∆°n gi·∫£n nh∆∞ sau b·∫±ng package torch: \\(f(x) = xw + b\\)\n\n\nCode\nx &lt;- torch_randn(100, 3)\nw &lt;- torch_randn(3, 1, requires_grad = TRUE)\nb &lt;- torch_zeros(1, 1, requires_grad = TRUE)\ny &lt;- x$matmul(w) + b\nhead(y)\n\n\ntorch_tensor\n-2.0308\n-0.9831\n 1.5817\n 2.4076\n-0.2181\n 1.1430\n[ CPUFloatType{6,1} ][ grad_fn = &lt;SliceBackward0&gt; ]\n\n\n\n\n\nTi·∫øp theo, m√¨nh s·∫Ω x√¢y d·ª±ng m√¥ h√¨nh Transformer ƒë·ªÉ d·ª± b√°o gi√° c·ªï phi·∫øu c·ªßa Google t·ª´ ngu·ªìn Yahoo Finance.\n\n\nCode\n#### Call packages-------------------------------------------------------------\npacman::p_load(quantmod,\n               torch,\n               dplyr,\n               dygraphs)\n#### Input---------------------------------------------------------------------\ngetSymbols(\"GOOG\", src = \"yahoo\", from = \"2020-01-01\", to = \"2022-01-01\")\n\n\n[1] \"GOOG\"\n\n\nCode\nprice_data &lt;- GOOG$GOOG.Close\nprice_data_xts &lt;- xts(price_data, \n                     order.by = index(price_data))\n\ncolors&lt;-RColorBrewer::brewer.pal(9, \"Blues\")[c(4, 6, 8)]\n\ndygraph(price_data_xts, main = \"Google Stock Price (2020 - 2022)\", ylab = \"Price ($)\") |&gt; \n  dyRangeSelector(height = 20) |&gt; \n  dyOptions(\n    fillGraph = TRUE,  \n    colors = colors,   \n    strokeWidth = 2,   \n    gridLineColor = \"gray\",  \n    gridLineWidth = 0.5,     \n    drawPoints = TRUE,   \n    pointSize = 4,       \n    pointShape = \"diamond\" \n  ) |&gt; \n  dyLegend(show = \"follow\") \n\n\n\n\n\n\nNh∆∞ bi·ªÉu ƒë·ªì, ta th·∫•y gi√° c·ªï phi·∫øu tƒÉng cao ch√≥ng m·∫∑t v√† m·ª©c bi·∫øn ƒë·ªông kh√° m·ª©c t·∫°p (l√∫c l√™n l√∫c xu·ªëng). Task n√†y kh√° kh√≥ n√™n ta s·∫Ω t√¨m hi·ªÉu xem performance c·ªßa m√¥ h√¨nh Transformer s·∫Ω nh∆∞ th·∫ø n√†o.\nM√¥ h√¨nh ƒë·∫ßy ƒë·ªß s·∫Ω ƒë∆∞·ª£c code nh∆∞ sau:\n\n\nShow structure\n#### Transform input----------------------------------------------------------------\ncreate_supervised_data &lt;- function(series, n) {\n  series &lt;- as.vector(series)\n  data &lt;- data.frame(series)\n  \n  for (i in 1:n) {\n    lagged_column &lt;- lag(series, i)\n    data &lt;- cbind(data, lagged_column)\n  }\n  \n  colnames(data) &lt;- c('t',paste0('t', 1:n))\n\n  data &lt;- na.omit(data)\n  \n  return(data)\n}\n\nseq_leng &lt;- 50\ndim_model &lt;- 32\n\nsupervised_data &lt;- create_supervised_data(price_data, n = seq_leng)\n\nsupervised_data &lt;- scale(supervised_data)\n\n\nx_data &lt;- torch_tensor(as.matrix(supervised_data[, 2:(seq_leng+1)]), dtype = torch_float())  # Features (lags)\ny_data &lt;- torch_tensor(as.matrix(supervised_data[, 1]), dtype = torch_float())    # Target\n\n# Reshape x_data to match (batch_size, seq_leng, feature_size)\nx_data &lt;- x_data$view(c(nrow(x_data), seq_leng, 1))  # (batch_size, seq_leng, feature_size)\ny_data &lt;- y_data$view(c(nrow(y_data), 1, 1)) \n\n# Split the data into training and testing sets (80% for training, 20% for testing)\ntrain_size &lt;- round(0.8 * nrow(supervised_data))\n\nx_train &lt;- x_data[1:train_size, , drop = FALSE]  \ny_train &lt;- y_data[1:train_size]\n\nx_test &lt;- x_data[(train_size + 1):nrow(supervised_data), , drop = FALSE]\ny_test &lt;- y_data[(train_size + 1):nrow(supervised_data)]\n\n#### Build components of model----------------------------------------------------------------\n### Positional encoding:\npositional_encoding &lt;- function(seq_leng, d_model, n = 10000) {\n  if (missing(seq_leng) || missing(d_model)) {\n    stop(\"'seq_leng' and 'd_model' must be provided.\")\n  }\n  \n  P &lt;- matrix(0, nrow = seq_leng, ncol = d_model)  \n  \n  for (k in 1:seq_leng) {\n    for (i in 0:(d_model / 2 - 1)) {\n      denominator &lt;- n^(2 * i / d_model)\n      P[k, 2 * i + 1] &lt;- sin(k / denominator)\n      P[k, 2 * i + 2] &lt;- cos(k / denominator)\n    }\n  }\n  \n  return(P)\n}\n\nen_pe &lt;- positional_encoding(x_data$size(2),dim_model, n = 10000)\nde_pe &lt;- positional_encoding(y_data$size(2),dim_model, n = 10000)\n\n### Encoder block:\nencoder_layer &lt;- nn_module(\n  \"TransformerEncoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    \n    # Multi-Head Attention\n    self$multihead_attention &lt;- nn_multihead_attention(embed_dim = d_model, num_heads = num_heads)\n    \n    # Feedforward Network (Fully Connected)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  \n  },\n  \n  forward = function(x) {\n\n    attn_output &lt;- self$multihead_attention(x, x, x) \n    x &lt;- x + attn_output[[1]]\n    x &lt;- self$layer_norm(x) \n    \n    # Feedforward network with residual connection\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Mask function:\nmask_self_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    # Ensure that self$head_dim is a scalar\n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    if (embed_dim %% num_heads != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    # Linear layers for Q, K, V \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n    # Final linear layer after concatenating heads\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n  },\n  \n  forward = function(x) {\n    batch_size &lt;- x$size(1)\n    seq_leng &lt;- x$size(2)\n    \n    # Linear projections for Q, K, V\n    Q &lt;- self$query(x)  # (batch_size, seq_leng, embed_dim)\n    K &lt;- self$key(x)\n    V &lt;- self$value(x)\n    \n    # Reshape to separate heads: (batch_size, num_heads, seq_leng, head_dim)\n    Q &lt;- Q$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    # Compute attention scores\n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    # Apply mask if provided\n    mask &lt;- torch_tril(torch_ones(c(seq_leng, seq_leng)))\n    \n    if (!is.null(mask)) {\n      \n      masked_attention_scores &lt;- attention_scores$masked_fill(mask == 0, -Inf)\n      \n    } else {\n      print(\"Warning: No mask provided\")\n    }\n    \n    # Compute attention weights\n    weights &lt;- nnf_softmax(masked_attention_scores, dim = -1)\n    \n    # Apply weights to V\n    attn_output &lt;- torch_matmul(weights, V)  # (batch_size, num_heads, seq_leng, head_dim)\n    \n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng, self$embed_dim))\n    \n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Cross attention:\ncross_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n  },\n  \n  forward = function(decoder_input, encoder_output, mask = NULL) {\n    batch_size &lt;- decoder_input$size(1)\n    seq_leng_dec &lt;- decoder_input$size(2)\n    seq_leng_enc &lt;- encoder_output$size(2)\n    \n    Q &lt;- self$query(decoder_input)\n    K &lt;- self$key(encoder_output)\n    V &lt;- self$value(encoder_output)\n    \n    Q &lt;- Q$view(c(batch_size, seq_leng_dec, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    weights &lt;- nnf_softmax(attention_scores, dim = -1)\n    \n    attn_output &lt;- torch_matmul(weights, V)\n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng_dec, self$embed_dim))\n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Decoder Layer\ndecoder_layer &lt;- nn_module(\n  \"TransformerDecoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    self$mask_self_attention &lt;- mask_self_attention(embed_dim = d_model, num_heads = num_heads)\n    self$cross_attention &lt;- cross_attention(embed_dim = d_model, num_heads = num_heads)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  },\n  \n  forward = function(x, encoder_output) {\n    # Masked Self-Attention\n    mask_output &lt;- self$mask_self_attention(x)\n    x &lt;- x + mask_output\n    x &lt;- self$layer_norm(x)\n    \n    # Encoder-Decoder Multi-Head Attention\n    cross_output &lt;- self$cross_attention(x, encoder_output)\n    x &lt;- x + cross_output\n    x &lt;- self$layer_norm(x)\n    \n    # Feedforward Network\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Final transformer model: \ntransformer &lt;- nn_module(\n  \"Transformer\",\n  \n  initialize = function(d_model, seq_leng, num_heads, d_ff, num_encoder_layers, num_decoder_layers) {\n    self$d_model &lt;- d_model\n    self$num_heads &lt;- num_heads\n    self$d_ff &lt;- d_ff\n    self$num_encoder_layers &lt;- num_encoder_layers\n    self$num_decoder_layers &lt;- num_decoder_layers\n    self$seq_leng &lt;- seq_leng\n    self$en_pe &lt;- en_pe\n    self$de_pe &lt;- de_pe\n    \n    # Encoder layers\n    self$encoder_layers &lt;- nn_module_list(\n      lapply(1:num_encoder_layers, function(i) {\n        encoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Decoder layers\n    self$decoder_layers &lt;- nn_module_list(\n      lapply(1:num_decoder_layers, function(i) {\n        decoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Final output layer\n    self$output_layer &lt;- nn_linear(d_model, 1)  # Output layer to predict a single value\n    \n  },\n  \n  forward = function(src, trg) {\n    \n    src &lt;- src + self$en_pe  \n    trg &lt;- trg + self$de_pe\n    \n    # Encoder forward pass\n    encoder_output &lt;- src\n    for (i in 1:self$num_encoder_layers) {\n      encoder_output &lt;- self$encoder_layers[[i]](encoder_output)\n    }\n    \n    # Decoder forward pass\n    decoder_output &lt;- trg\n    for (i in 1:self$num_decoder_layers) {\n      decoder_output &lt;- self$decoder_layers[[i]](decoder_output, encoder_output)\n    }\n  \n    # Apply final output layer\n    output &lt;- self$output_layer(decoder_output)\n    \n    return(output)\n  }\n)\n\n#### Training----------------------------------------------------------------\nmodel &lt;- transformer(\n  d_model = dim_model,         # Embedding dimension\n  seq_leng = seq_leng,        # Sequence length\n  num_heads = 8,        # Number of heads\n  d_ff = seq_leng,           # Dimension of the feedforward layer\n  num_encoder_layers = 6, \n  num_decoder_layers = 6\n)\n\n\n#### Training----------------------------------------------------------------\nepochs &lt;- 200\nloss_fn &lt;- nn_mse_loss()\noptimizer &lt;- optim_adam(model$parameters, lr = 1e-3)\n\nfor (epoch in 1:epochs) {\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_train_pred &lt;- model(x_train, y_train) \n  \n  # Compute the loss\n  loss &lt;- loss_fn(y_train_pred, y_train)\n  \n  # Backpropagation and optimization\n  loss$backward()\n  optimizer$step()\n  \n  if (epoch %% 10 == 0) {\n    cat(\"Epoch: \", epoch, \" Loss: \", loss$item(), \"\\n\")\n  }\n}\n\n#### Predictions----------------------------------------------------------------\nmodel$eval()\n\n# Make predictions on the test data\ny_test_pred &lt;- model(x_test, y_test)  # Use the test data for both input and output during prediction\n\n# Convert tensors to numeric values for comparison\n\ny_test_pred&lt;- as.numeric(as.array(y_test_pred$cpu()))\n\n#### Evaluating----------------------------------------------------------------\nlibrary(highcharter)\ny_train_pred &lt;- as.numeric(as.array(y_train_pred$cpu()))\ny_train &lt;- as.numeric(as.array(y_train$cpu()))\ny_test &lt;- as.numeric(as.array(y_test$cpu()))\n\ncomparison &lt;- data.frame(\n  time = 1:nrow(supervised_data),\n  actual = c(y_train,y_test),\n  forecast = c(y_train_pred,y_test_pred)\n)\n\n# Compare only errors:\nerror&lt;-highchart() |&gt;\n  hc_title(text = \"Evaluating error of model\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Error\",\n    data = (y_test_pred - y_test)/y_test,\n    type = \"line\",\n    color = \"red\"  # Blue color for actual data\n  ) |&gt;\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n# Compare all:\nall&lt;-highchart() |&gt;\n  hc_title(text = \"Model Predictions vs Actual Values\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Actual Data\",\n    data = comparison$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) |&gt;\n  hc_add_series(\n    name = \"Forecast\",\n    data = comparison$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) |&gt; \n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\nƒê·∫ßu ti√™n ta s·∫Ω ƒë√°nh gi√° v·ªÅ sai s·ªë c·ªßa m√¥ h√¨nh khi d√πng testing data. K·∫øt qu·∫£ kh√° ·ªïn khi sai s·ªë kho·∫£ng (0.04,0.12).\n\n  \n\nV√† c√≤n nh√¨n t·ªïng quan h·∫øt th√¨ ta th·∫•y m√¥ h√¨nh d·ª± ƒëo√°n kh√° s√°t v·ªõi training data nh∆∞ng v·ªõi testing data th√¨ v·∫´n ch√™nh l·ªách th·∫•p h∆°n th·ª±c t·∫ø (d·∫•u hi·ªáu cho th·∫•y m√¥ h√¨nh ƒëang b·ªã overfitting)."
  },
  {
    "objectID": "practice.html#th·ª±c-h√†nh-trong-r",
    "href": "practice.html#th·ª±c-h√†nh-trong-r",
    "title": "Time series forecasting",
    "section": "",
    "text": "Khi b·∫°n c·∫ßn x√¢y d·ª±ng c√°c m√¥ h√¨nh Deep learning ph·ª©c t·∫°p h∆°n th√¨ package torch trong R gi·ªëng v·ªõi PyTorch trong Python th∆∞·ªùng d√πng ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh machine learning v√† n√≥ s·∫Ω l√† c√¥ng c·ª• m·∫°nh m·∫Ω c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n (N·∫øu b·∫°n ch∆∞a bi·∫øt th√¨ h·∫ßu h·∫øt packages ƒë·ªÉ train machine learning model trong R ƒë·ªÅu th·ª±c hi·ªán th√¥ng qua Python v√† ƒë∆∞·ª£c b·∫Øt c·∫ßu n·ªëi b·∫±ng package reticulate).\nƒê·ªÉ h·ªçc h·∫øt v·ªÅ torch th√¨ b·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c link sau:\n\nS√°ch Deep Learning and Scientific Computing with R torch c·ªßa Sigrid Keydana.\nM·ªôt lo·∫°t b√†i post t·ª´ posit blog.\n\nC√≤n ·ªü b√†i vi·∫øt n√†y, m√¨nh ch·ªâ gi·ªõi thi·ªáu c∆° b·∫£n c√°ch s·ª≠ d·ª•ng torch trong R ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh.\n\n\n\n\n\n\nTip\n\n\n\nƒê·ªÉ t·∫£i torch v√†o m√°y local th√¨ b·∫°n s·ª≠ d·ª•ng c√∫ ph√°p install.package(\"torch\")\n\n\n\n\n·ªû ƒë√¢y, m√¨nh s·∫Ω gi·ªõi thi·ªáu c∆° b·∫£n ƒë·ªÉ m·ªçi ng∆∞·ªùi c√≥ ki·∫øn th·ª©c c∆° b·∫£n nh·∫•t v·ªÅ torch\nƒê·∫ßu ti√™n, ƒë·ªÉ d√πng package torch trong R th√¨ ta c·∫ßn chuy·ªÉn ƒë·ªïi object sang class tensor th√¨ d√πng h√†m torch_tensor(). ƒê√¢y l√† v√≠ d·ª• v·ªÅ:\n\n\nCode\nlibrary(torch)\nm&lt;-torch_tensor(array(1:24, dim = c(4, 3, 2)))\nclass(m)\n\n\n[1] \"torch_tensor\" \"R7\"          \n\n\nNgo√†i ra, trong object tensor c√≤n ch·ª©a th√™m th√¥ng tin kh√°c nh∆∞ l√†: $dtype s·∫Ω return data type (v√≠ d·ª• nh∆∞ object d∆∞·ªõi ƒë√¢y l√† d·∫°ng long integer), $device return n∆°i tensor object ƒë∆∞·ª£c l∆∞u tr·ªØ, $shape return dimensions c·ªßa object.\n\n\nCode\nm$dtype\n\n\ntorch_Long\n\n\nCode\nm$device\n\n\ntorch_device(type='cpu') \n\n\nCode\nm$shape\n\n\n[1] 4 3 2\n\n\nV√≠ d·ª• ta c√≥ th·ªÉ simulate c√¥ng th·ª©c ƒë∆°n gi·∫£n nh∆∞ sau b·∫±ng package torch: \\(f(x) = xw + b\\)\n\n\nCode\nx &lt;- torch_randn(100, 3)\nw &lt;- torch_randn(3, 1, requires_grad = TRUE)\nb &lt;- torch_zeros(1, 1, requires_grad = TRUE)\ny &lt;- x$matmul(w) + b\nhead(y)\n\n\ntorch_tensor\n-2.0308\n-0.9831\n 1.5817\n 2.4076\n-0.2181\n 1.1430\n[ CPUFloatType{6,1} ][ grad_fn = &lt;SliceBackward0&gt; ]\n\n\n\n\n\nTi·∫øp theo, m√¨nh s·∫Ω x√¢y d·ª±ng m√¥ h√¨nh Transformer ƒë·ªÉ d·ª± b√°o gi√° c·ªï phi·∫øu c·ªßa Google t·ª´ ngu·ªìn Yahoo Finance.\n\n\nCode\n#### Call packages-------------------------------------------------------------\npacman::p_load(quantmod,\n               torch,\n               dplyr,\n               dygraphs)\n#### Input---------------------------------------------------------------------\ngetSymbols(\"GOOG\", src = \"yahoo\", from = \"2020-01-01\", to = \"2022-01-01\")\n\n\n[1] \"GOOG\"\n\n\nCode\nprice_data &lt;- GOOG$GOOG.Close\nprice_data_xts &lt;- xts(price_data, \n                     order.by = index(price_data))\n\ncolors&lt;-RColorBrewer::brewer.pal(9, \"Blues\")[c(4, 6, 8)]\n\ndygraph(price_data_xts, main = \"Google Stock Price (2020 - 2022)\", ylab = \"Price ($)\") |&gt; \n  dyRangeSelector(height = 20) |&gt; \n  dyOptions(\n    fillGraph = TRUE,  \n    colors = colors,   \n    strokeWidth = 2,   \n    gridLineColor = \"gray\",  \n    gridLineWidth = 0.5,     \n    drawPoints = TRUE,   \n    pointSize = 4,       \n    pointShape = \"diamond\" \n  ) |&gt; \n  dyLegend(show = \"follow\") \n\n\n\n\n\n\nNh∆∞ bi·ªÉu ƒë·ªì, ta th·∫•y gi√° c·ªï phi·∫øu tƒÉng cao ch√≥ng m·∫∑t v√† m·ª©c bi·∫øn ƒë·ªông kh√° m·ª©c t·∫°p (l√∫c l√™n l√∫c xu·ªëng). Task n√†y kh√° kh√≥ n√™n ta s·∫Ω t√¨m hi·ªÉu xem performance c·ªßa m√¥ h√¨nh Transformer s·∫Ω nh∆∞ th·∫ø n√†o.\nM√¥ h√¨nh ƒë·∫ßy ƒë·ªß s·∫Ω ƒë∆∞·ª£c code nh∆∞ sau:\n\n\nShow structure\n#### Transform input----------------------------------------------------------------\ncreate_supervised_data &lt;- function(series, n) {\n  series &lt;- as.vector(series)\n  data &lt;- data.frame(series)\n  \n  for (i in 1:n) {\n    lagged_column &lt;- lag(series, i)\n    data &lt;- cbind(data, lagged_column)\n  }\n  \n  colnames(data) &lt;- c('t',paste0('t', 1:n))\n\n  data &lt;- na.omit(data)\n  \n  return(data)\n}\n\nseq_leng &lt;- 50\ndim_model &lt;- 32\n\nsupervised_data &lt;- create_supervised_data(price_data, n = seq_leng)\n\nsupervised_data &lt;- scale(supervised_data)\n\n\nx_data &lt;- torch_tensor(as.matrix(supervised_data[, 2:(seq_leng+1)]), dtype = torch_float())  # Features (lags)\ny_data &lt;- torch_tensor(as.matrix(supervised_data[, 1]), dtype = torch_float())    # Target\n\n# Reshape x_data to match (batch_size, seq_leng, feature_size)\nx_data &lt;- x_data$view(c(nrow(x_data), seq_leng, 1))  # (batch_size, seq_leng, feature_size)\ny_data &lt;- y_data$view(c(nrow(y_data), 1, 1)) \n\n# Split the data into training and testing sets (80% for training, 20% for testing)\ntrain_size &lt;- round(0.8 * nrow(supervised_data))\n\nx_train &lt;- x_data[1:train_size, , drop = FALSE]  \ny_train &lt;- y_data[1:train_size]\n\nx_test &lt;- x_data[(train_size + 1):nrow(supervised_data), , drop = FALSE]\ny_test &lt;- y_data[(train_size + 1):nrow(supervised_data)]\n\n#### Build components of model----------------------------------------------------------------\n### Positional encoding:\npositional_encoding &lt;- function(seq_leng, d_model, n = 10000) {\n  if (missing(seq_leng) || missing(d_model)) {\n    stop(\"'seq_leng' and 'd_model' must be provided.\")\n  }\n  \n  P &lt;- matrix(0, nrow = seq_leng, ncol = d_model)  \n  \n  for (k in 1:seq_leng) {\n    for (i in 0:(d_model / 2 - 1)) {\n      denominator &lt;- n^(2 * i / d_model)\n      P[k, 2 * i + 1] &lt;- sin(k / denominator)\n      P[k, 2 * i + 2] &lt;- cos(k / denominator)\n    }\n  }\n  \n  return(P)\n}\n\nen_pe &lt;- positional_encoding(x_data$size(2),dim_model, n = 10000)\nde_pe &lt;- positional_encoding(y_data$size(2),dim_model, n = 10000)\n\n### Encoder block:\nencoder_layer &lt;- nn_module(\n  \"TransformerEncoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    \n    # Multi-Head Attention\n    self$multihead_attention &lt;- nn_multihead_attention(embed_dim = d_model, num_heads = num_heads)\n    \n    # Feedforward Network (Fully Connected)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  \n  },\n  \n  forward = function(x) {\n\n    attn_output &lt;- self$multihead_attention(x, x, x) \n    x &lt;- x + attn_output[[1]]\n    x &lt;- self$layer_norm(x) \n    \n    # Feedforward network with residual connection\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Mask function:\nmask_self_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    # Ensure that self$head_dim is a scalar\n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    if (embed_dim %% num_heads != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    # Linear layers for Q, K, V \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n    # Final linear layer after concatenating heads\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n  },\n  \n  forward = function(x) {\n    batch_size &lt;- x$size(1)\n    seq_leng &lt;- x$size(2)\n    \n    # Linear projections for Q, K, V\n    Q &lt;- self$query(x)  # (batch_size, seq_leng, embed_dim)\n    K &lt;- self$key(x)\n    V &lt;- self$value(x)\n    \n    # Reshape to separate heads: (batch_size, num_heads, seq_leng, head_dim)\n    Q &lt;- Q$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    # Compute attention scores\n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    # Apply mask if provided\n    mask &lt;- torch_tril(torch_ones(c(seq_leng, seq_leng)))\n    \n    if (!is.null(mask)) {\n      \n      masked_attention_scores &lt;- attention_scores$masked_fill(mask == 0, -Inf)\n      \n    } else {\n      print(\"Warning: No mask provided\")\n    }\n    \n    # Compute attention weights\n    weights &lt;- nnf_softmax(masked_attention_scores, dim = -1)\n    \n    # Apply weights to V\n    attn_output &lt;- torch_matmul(weights, V)  # (batch_size, num_heads, seq_leng, head_dim)\n    \n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng, self$embed_dim))\n    \n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Cross attention:\ncross_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n  },\n  \n  forward = function(decoder_input, encoder_output, mask = NULL) {\n    batch_size &lt;- decoder_input$size(1)\n    seq_leng_dec &lt;- decoder_input$size(2)\n    seq_leng_enc &lt;- encoder_output$size(2)\n    \n    Q &lt;- self$query(decoder_input)\n    K &lt;- self$key(encoder_output)\n    V &lt;- self$value(encoder_output)\n    \n    Q &lt;- Q$view(c(batch_size, seq_leng_dec, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    weights &lt;- nnf_softmax(attention_scores, dim = -1)\n    \n    attn_output &lt;- torch_matmul(weights, V)\n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng_dec, self$embed_dim))\n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Decoder Layer\ndecoder_layer &lt;- nn_module(\n  \"TransformerDecoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    self$mask_self_attention &lt;- mask_self_attention(embed_dim = d_model, num_heads = num_heads)\n    self$cross_attention &lt;- cross_attention(embed_dim = d_model, num_heads = num_heads)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  },\n  \n  forward = function(x, encoder_output) {\n    # Masked Self-Attention\n    mask_output &lt;- self$mask_self_attention(x)\n    x &lt;- x + mask_output\n    x &lt;- self$layer_norm(x)\n    \n    # Encoder-Decoder Multi-Head Attention\n    cross_output &lt;- self$cross_attention(x, encoder_output)\n    x &lt;- x + cross_output\n    x &lt;- self$layer_norm(x)\n    \n    # Feedforward Network\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Final transformer model: \ntransformer &lt;- nn_module(\n  \"Transformer\",\n  \n  initialize = function(d_model, seq_leng, num_heads, d_ff, num_encoder_layers, num_decoder_layers) {\n    self$d_model &lt;- d_model\n    self$num_heads &lt;- num_heads\n    self$d_ff &lt;- d_ff\n    self$num_encoder_layers &lt;- num_encoder_layers\n    self$num_decoder_layers &lt;- num_decoder_layers\n    self$seq_leng &lt;- seq_leng\n    self$en_pe &lt;- en_pe\n    self$de_pe &lt;- de_pe\n    \n    # Encoder layers\n    self$encoder_layers &lt;- nn_module_list(\n      lapply(1:num_encoder_layers, function(i) {\n        encoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Decoder layers\n    self$decoder_layers &lt;- nn_module_list(\n      lapply(1:num_decoder_layers, function(i) {\n        decoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Final output layer\n    self$output_layer &lt;- nn_linear(d_model, 1)  # Output layer to predict a single value\n    \n  },\n  \n  forward = function(src, trg) {\n    \n    src &lt;- src + self$en_pe  \n    trg &lt;- trg + self$de_pe\n    \n    # Encoder forward pass\n    encoder_output &lt;- src\n    for (i in 1:self$num_encoder_layers) {\n      encoder_output &lt;- self$encoder_layers[[i]](encoder_output)\n    }\n    \n    # Decoder forward pass\n    decoder_output &lt;- trg\n    for (i in 1:self$num_decoder_layers) {\n      decoder_output &lt;- self$decoder_layers[[i]](decoder_output, encoder_output)\n    }\n  \n    # Apply final output layer\n    output &lt;- self$output_layer(decoder_output)\n    \n    return(output)\n  }\n)\n\n#### Training----------------------------------------------------------------\nmodel &lt;- transformer(\n  d_model = dim_model,         # Embedding dimension\n  seq_leng = seq_leng,        # Sequence length\n  num_heads = 8,        # Number of heads\n  d_ff = seq_leng,           # Dimension of the feedforward layer\n  num_encoder_layers = 6, \n  num_decoder_layers = 6\n)\n\n\n#### Training----------------------------------------------------------------\nepochs &lt;- 200\nloss_fn &lt;- nn_mse_loss()\noptimizer &lt;- optim_adam(model$parameters, lr = 1e-3)\n\nfor (epoch in 1:epochs) {\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_train_pred &lt;- model(x_train, y_train) \n  \n  # Compute the loss\n  loss &lt;- loss_fn(y_train_pred, y_train)\n  \n  # Backpropagation and optimization\n  loss$backward()\n  optimizer$step()\n  \n  if (epoch %% 10 == 0) {\n    cat(\"Epoch: \", epoch, \" Loss: \", loss$item(), \"\\n\")\n  }\n}\n\n#### Predictions----------------------------------------------------------------\nmodel$eval()\n\n# Make predictions on the test data\ny_test_pred &lt;- model(x_test, y_test)  # Use the test data for both input and output during prediction\n\n# Convert tensors to numeric values for comparison\n\ny_test_pred&lt;- as.numeric(as.array(y_test_pred$cpu()))\n\n#### Evaluating----------------------------------------------------------------\nlibrary(highcharter)\ny_train_pred &lt;- as.numeric(as.array(y_train_pred$cpu()))\ny_train &lt;- as.numeric(as.array(y_train$cpu()))\ny_test &lt;- as.numeric(as.array(y_test$cpu()))\n\ncomparison &lt;- data.frame(\n  time = 1:nrow(supervised_data),\n  actual = c(y_train,y_test),\n  forecast = c(y_train_pred,y_test_pred)\n)\n\n# Compare only errors:\nerror&lt;-highchart() |&gt;\n  hc_title(text = \"Evaluating error of model\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Error\",\n    data = (y_test_pred - y_test)/y_test,\n    type = \"line\",\n    color = \"red\"  # Blue color for actual data\n  ) |&gt;\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n# Compare all:\nall&lt;-highchart() |&gt;\n  hc_title(text = \"Model Predictions vs Actual Values\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Actual Data\",\n    data = comparison$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) |&gt;\n  hc_add_series(\n    name = \"Forecast\",\n    data = comparison$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) |&gt; \n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\nƒê·∫ßu ti√™n ta s·∫Ω ƒë√°nh gi√° v·ªÅ sai s·ªë c·ªßa m√¥ h√¨nh khi d√πng testing data. K·∫øt qu·∫£ kh√° ·ªïn khi sai s·ªë kho·∫£ng (0.04,0.12).\n\n  \n\nV√† c√≤n nh√¨n t·ªïng quan h·∫øt th√¨ ta th·∫•y m√¥ h√¨nh d·ª± ƒëo√°n kh√° s√°t v·ªõi training data nh∆∞ng v·ªõi testing data th√¨ v·∫´n ch√™nh l·ªách th·∫•p h∆°n th·ª±c t·∫ø (d·∫•u hi·ªáu cho th·∫•y m√¥ h√¨nh ƒëang b·ªã overfitting)."
  },
  {
    "objectID": "practice.html#k·∫øt-lu·∫≠n",
    "href": "practice.html#k·∫øt-lu·∫≠n",
    "title": "Time series forecasting",
    "section": "2 K·∫øt lu·∫≠n:",
    "text": "2 K·∫øt lu·∫≠n:\nNh∆∞ v·∫≠y ta ƒë√£ th·∫•y ƒë∆∞·ª£c s·ª©c m·∫°nh c·ªßa m√¥ h√¨nh Transformer trong d·ª± b√°o cho d·ªØ li·ªáu sequence (m·∫∑c d√π m√¨nh mong mu·ªën error rate &lt; 0.05 nh∆∞ng k·∫øt qu·∫£ v·∫´n ch·∫•p nh·∫≠n ƒë∆∞·ª£c).\nM·ªôt s·ªë suggestion c·ªßa m√¨nh cho m√¥ h√¨nh Transformer ƒë·ªÉ improve performance nh∆∞ sau:\n\nTh√™m layer nn_dropout(p) v√†o m√¥ h√¨nh: l√† m·ªôt ph∆∞∆°ng ph√°p regularization (chu·∫©n h√≥a) ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m·∫°ng n∆°-ron ƒë·ªÉ ngƒÉn ng·ª´a hi·ªán t∆∞·ª£ng overfitting (qu√° kh·ªõp) b·∫±ng c√°ch ng·∫´u nhi√™n ‚Äúlo·∫°i b·ªè‚Äù m·ªôt t·ª∑ l·ªá ph·∫ßn trƒÉm n∆°-ron trong qu√° tr√¨nh hu·∫•n luy·ªán. B·∫°n ch·ªâ c·∫ßn th√™m ƒë·ªëi s·ªë p l√† t·ª∑ l·ªá % dropout.\nD√πng c√°c variant c·ªßa Transformer: th·ª±c ch·∫•t m·ª•c ƒë√≠ch ban ƒë·∫ßu c·ªßa Transformer l√† deal v·ªõi c√°c tasks li√™n quan v·ªÅ d·ªãch thu·∫≠t, x·ª≠ l√≠ vƒÉn b·∫£n, ph√¢n t√≠ch h√¨nh ·∫£nh,‚Ä¶ ch·ª© kh√¥ng thi√™n v·ªÅ time series forecasting. M√¥ h√¨nh deep learning kh√°c thi√™n v·ªÅ v·∫•n ƒë·ªÅ n√†y m√† b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng l√† Informer.\n\nN·∫øu b·∫°n c√≥ c√¢u h·ªèi hay th·∫Øc m·∫Øc n√†o, ƒë·ª´ng ng·∫ßn ng·∫°i li√™n h·ªá v·ªõi m√¨nh qua Gmail. B√™n c·∫°nh ƒë√≥, n·∫øu b·∫°n mu·ªën xem l·∫°i c√°c b√†i vi·∫øt tr∆∞·ªõc ƒë√¢y c·ªßa m√¨nh, h√£y nh·∫•n v√†o hai n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ truy c·∫≠p trang Rpubs ho·∫∑c m√£ ngu·ªìn tr√™n Github. R·∫•t vui ƒë∆∞·ª£c ƒë·ªìng h√†nh c√πng b·∫°n, h·∫πn g·∫∑p l·∫°i! üòÑüòÑüòÑ\n\n\n\n    \n    \n    Contact Me\n    \n    \n    \n\n\n    \n        Contact Me\n        \n            Your Email:\n            \n            Please enter a valid email address.\n            Send Email\n        \n        \n            \n                \n                     View Code on GitHub\n                \n            \n        \n        \n            \n                \n                     Visit my RPubs"
  },
  {
    "objectID": "transfer.html",
    "href": "transfer.html",
    "title": "M√¥ h√¨nh Transformer",
    "section": "",
    "text": "Trong m·ªôt nghi√™n c·ª©u c·ªßa (Jimeng Shi, Mahek Jain, and Giri Narasimhan 2022) v·ªÅ vi·ªác ·ª©ng d·ª•ng h√†ng lo·∫°t c√°c m√¥ h√¨nh thu·ªôc ph√¢n l·ªõp Deep learning v√† so s√°nh ƒë·ªÉ ch·ªçn ra m√¥ h√¨nh d·ª± ƒëo√°n t·ªët nh·∫•t ch·ªâ s·ªë PM2.5 (l√† ch·ªâ s·ªë ƒëo l∆∞·ªùng l∆∞·ª£ng h·∫°t b·ª•i li ti c√≥ trong kh√¥ng kh√≠ v·ªõi k√≠ch th∆∞·ªõc 2,5 micron tr·ªü xu·ªëng). K·∫øt qu·∫£ c√≥ bao g·ªìm: ‚ÄúM√¥ h√¨nh Transformer d·ª± ƒëo√°n t·ªët nh·∫•t cho d·ª± ƒëo√°n long-term trong t∆∞∆°ng lai. LSTM v√† GRU v∆∞·ª£t tr·ªôi h∆°n RNN cho c√°c d·ª± ƒëo√°n short-term.‚Äù\nV·∫≠y m√¥ h√¨nh Transformer l√† g√¨ ? Ch√∫ng ta s·∫Ω h·ªçc n√≥ ·ªü b√†i n√†y."
  },
  {
    "objectID": "transfer.html#m√¥-h√¨nh-transformer",
    "href": "transfer.html#m√¥-h√¨nh-transformer",
    "title": "M√¥ h√¨nh Transformer",
    "section": "1 M√¥ h√¨nh Transformer:",
    "text": "1 M√¥ h√¨nh Transformer:\n\n1.1 Gi·ªõi thi·ªáu:\n\n\nCode\npacman::p_load(torch,\n               dplyr,\n               tidyverse)\n\n\nCh·∫Øc c√°c b·∫°n ƒë√£ qu√° quen thu·ªôc v·ªõi Chatgpt - m·ªôt c√¥ng c·ª• AI m·∫°nh m·∫Ω trong th·ªùi gian g·∫ßn ƒë√¢y v·ªõi l∆∞·ª£ng ng∆∞·ªùi s·ª≠ d·ª•ng c·ª±c k√¨ cao. Nh∆∞ bi·ªÉu ƒë·ªì d∆∞·ªõi ƒë√¢y, t·ª´ khi launched Chatgpt ch·ªâ t·ªën 5 ng√†y ƒë·ªÉ ƒë·∫°t 1 tri·ªáu ng∆∞·ªùi s·ª≠ d·ª•ng v√† ngo√†i ra theo th·ªëng k√™ ƒë·∫øn th√°ng 2/2024, Chatgpt ƒë√£ c√≥ t·ªõi 1.6 t·ªâ l∆∞·ª£t thƒÉm quan.\n\n  \n  \n  \n  \n    H√¨nh 1: Th·ªùi gian ƒë·ªÉ ƒë·∫°t 1 tri·ªáu ng∆∞·ªùi d√πng c·ªßa Chatgpt\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n√ù t∆∞·ªüng ban ƒë·∫ßu c·ªßa Chatgpt ch√≠nh l√† d·ª±a tr√™n c·∫•u tr√∫c m√¥ h√¨nh Transformer - 1 d·∫°ng Deep learning ch·ªâ m·ªõi ƒë∆∞·ª£c gi·ªõi thi·ªáu v·ªõi th·∫ø gi·ªõi t·ª´ nƒÉm 2017 nh∆∞ng c√≥ s·ª©c ·∫£nh h∆∞·ªüng r·∫•t l·ªõn, nh·∫•t l√† trong lƒ©nh v·ª±c Generative AI.\nKh√°i ni·ªám v·ªÅ m√¥ h√¨nh n√†y ƒë∆∞·ª£c gi·ªõi thi·ªáu l·∫ßn ƒë·∫ßu v√†o nƒÉm 2017 c·ªßa c√°c nh√† nghi√™n c·ª©u c·ªßa Google trong b√†i t√†i li·ªáu Attention is all you need. M√¥ h√¨nh n√†y d·ª±a tr√™n √Ω t∆∞·ªüng l√† x√°c ƒë·ªãnh c√°c th√†nh ph·∫ßn quan tr·ªçng trong sequence v√† cho ph√©p m√¥ h√¨nh ƒë∆∞a ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n s·ª± ph·ª• thu·ªôc gi·ªØa c√°c ph·∫ßn t·ª≠ trong ƒë·∫ßu v√†o, b·∫•t k·ªÉ kho·∫£ng c√°ch c·ªßa ch√∫ng v·ªõi nhau, qu√° tr√¨nh n√†y g·ªçi l√† Attention mechanisms. D·ª±a v√†o ƒë√≥, m√¥ h√¨nh Transformer s·∫Ω chuy·ªÉn ƒë·ªïi m·ªôt chu·ªói input th√†nh 1 chu·ªói output kh√°c nh∆∞ng v·∫´n ƒë·∫£m b·∫£o gi·ªØ l·∫°i c√°c ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng c·ªßa sequence ƒë√≥.\n\n  \n  \n  \n  \n    H√¨nh 2: Input v√† output c·ªßa m√¥ h√¨nh\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nV√≠ d·ª• v·ªõi vi·ªác d·ªãch thu·∫≠t vƒÉn b·∫£n s·∫Ω c√≥ nh·ªØng t·ª´ trong c√¢u, c√¢u trong ƒëo·∫°n vƒÉn ƒë·∫°i di·ªán cho √Ω nghƒ©a to√†n c√¢u, to√†n ƒëo·∫°n vƒÉn. Hay v·ªõi v·ªÅ vi·ªác ph√¢n t√≠ch demand trong time series, l∆∞·ª£ng mua h√†ng v√†o nh·ªØng ng√†y ngh·ªâ, cu·ªëi tu·∫ßn s·∫Ω ƒë∆∞a ra insight t·ªët h∆°n v√†o c√°c ng√†y b√¨nh th∆∞·ªùng. Nh∆∞ v·∫≠y, b·∫°n th·∫•y ƒë√≥, m√¥ h√¨nh Transformer ph√π h·ª£p v·ªõi c√°c task thu·ªôc d·∫°ng d·ªãch vƒÉn b·∫£n, d·ª± ƒëo√°n chu·ªói h√†nh ƒë·ªông li√™n ti·∫øp c·ªßa ƒë·ªëi t∆∞·ª£ng,‚Ä¶\n\n\n1.2 So s√°nh v·ªõi RNN, LSTM:\nNh∆∞ h√¨nh tr√™n, b·∫°n c√≥ th·ªÉ th·∫•y m√¥ h√¨nh Transformer c≈©ng g·ªìm Encoder v√† Decoder gi·ªëng nh∆∞ c√°ch ho·∫°t ƒë·ªông c·ªßa RNN, LSTM. Nh∆∞ng kh√°c nhau ·ªü ch·ªó, thay v√¨ c∆° ch·∫ø ƒë√≥ ho·∫°t ƒë·ªông ·ªü t·ª´ng timestep li√™n t·ª•c nhau nh∆∞ RNN th√¨ ·ªü Transformer input ƒë∆∞·ª£c ƒë·∫©y v√†o c√πng 1 l√∫c (nghƒ©a l√† kh√¥ng c√≤n h·ªçc theo t·ª´ng timestep n·ªØa). Nh·ªù v·∫≠y, Transformer s·∫Ω x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c th√†nh ph·∫ßn quan tr·ªçng trong sequence v√† l·ª±a ch·ªçn th√¥ng s·ªë cho ch√∫ng (Hi·ªÉu ƒë∆°n gi·∫£n nh∆∞ vi·ªác b·∫°n c·∫ßn nghe h·∫øt ƒëo·∫°n tho·∫°i c·ªßa ng∆∞·ªùi ƒë·ªëi di·ªán th√¨ m·ªõi hi·ªÉu ƒë∆∞·ª£c h·ªç ƒëang n√≥i g√¨ v√† ch·ªçn l·ªçc c√°c keyword ƒë·ªÉ x√°c ƒë·ªãnh √Ω ch√≠nh c·ªßa ƒëo·∫°n vƒÉn ƒë√≥ v√† ƒë√≥ l√† √Ω t∆∞·ªüng ch√≠nh x√¢y d·ª±ng l√™n m√¥ h√¨nh n√†y).\n\n  \n  \n  \n  \n    H√¨nh 3: So s√°nh performance gi·ªØa m√¥ h√¨nh Transformer v√† LSTM\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nNgo√†i ra, ch√≠nh c∆° ch·∫ø Self-attention ƒë√£ t·∫°o s·ª± kh√°c bi·ªát l·ªõn cho m√¥ h√¨nh Transformer so v·ªõi c√°c m√¥ h√¨nh kh√°c. Nh∆∞ h√¨nh d∆∞·ªõi ƒë√¢y l√† nghi√™n c·ª©u c·ªßa v·ªÅ vi·ªác ·ª©ng d·ª•ng Deep learning ƒë·ªÉ t·∫°o ph·ª• ƒë·ªÅ cho video. Nghi√™n c·ª©u ƒë√£ so s√°nh performance gi·ªØa 2 m√¥ h√¨nh (i) Transformer-based model v√† (ii) LSTM-based model khi hyperparamater tuning. K·∫øt qu·∫£ cho th·∫•y s·ª± v∆∞·ª£t tr·ªôi c·ªßa Transformer khi ch·ªâ s·ªë accuracy l√™n t·ªõi 97%.\nTi·∫øp theo, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu v·ªÅ c√°c th√†nh ph·∫ßn ch√≠nh trong m√¥ h√¨nh Transformer."
  },
  {
    "objectID": "transfer.html#c√°c-th√†nh-ph·∫ßn-c∆°-b·∫£n-trong-transformer",
    "href": "transfer.html#c√°c-th√†nh-ph·∫ßn-c∆°-b·∫£n-trong-transformer",
    "title": "M√¥ h√¨nh Transformer",
    "section": "2 C√°c th√†nh ph·∫ßn c∆° b·∫£n trong Transformer:",
    "text": "2 C√°c th√†nh ph·∫ßn c∆° b·∫£n trong Transformer:\nV·ªÅ nguy√™n l√≠ ho·∫°t ƒë·ªông, m√¨nh s·∫Ω chia th√†nh c√°c ph·∫ßn nh∆∞ sau theo c√°ch gi·∫£i th√≠ch c√° nh√¢n ƒë·ªÉ gi√∫p m·ªçi ng∆∞·ªùi d·ªÖ hi·ªÉu:\n\nTh√†nh ph·∫ßn 1: Tensor\n\nƒê·∫ßu ti√™n, c√°c b·∫°n ph·∫£i hi·ªÉu v·ªÅ tensor l√† g√¨? Th√¨ n√≥ l√† m·ªôt ƒë·ªëi t∆∞·ª£ng to√°n h·ªçc nh·∫±m t·ªïng h·ª£p h√≥a 1 ho·∫∑c nhi·ªÅu chi·ªÅu trong 1 object. D·∫°ng ƒë∆°n gi·∫£n c·ªßa tensor nh∆∞ l√† scalar (s·ªë ƒë∆°n gi·∫£n), vector (chu·ªói c√°c s·ªë),‚Ä¶\n\n  \n  \n  \n  \n    H√¨nh 4: Tensor l√† g√¨\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nV√† m·ª•c ƒë√≠ch c·ªßa vi·ªác chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang d·∫°ng tensor l√† ƒë·ªÉ gi√∫p cho vi·ªác t√≠nh to√°n tr√™n GPU nhanh h∆°n v√† tƒÉng t·ªëc ƒë·ªô training machine learning model. Ngo√†i ra, v·∫´n c√≥ c√°c th√¥ng tin kh√°c hay v·ªÅ tensor trong R, b·∫°n c√≥ th·ªÉ kham kh·∫£o link n√†y: Tensors.\n\nTh√†nh ph·∫ßn 2: Embedding v√† positional encoding\n\nKhi d·ªØ li·ªáu ƒë∆∞·ª£c ƒë∆∞a v√†o, n√≥ s·∫Ω tr·∫£i qua b∆∞·ªõc embedding (c√°ch ƒë·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu ƒëa chi·ªÅu trong kh√¥ng gian √≠t chi·ªÅu). N·∫øu d·ªØ li·ªáu c·ªßa b·∫°n d·∫°ng h√¨nh ·∫£nh ho·∫∑c d·∫°ng vƒÉn b·∫£n th√¨ b∆∞·ªõc n√†y r·∫•t c·∫ßn thi·∫øt (v√¨ c√°c m√¥ h√¨nh machine learning ch·ªâ l√†m vi·ªác ƒë∆∞·ª£c v·ªõi d·ªØ li·ªáu d·∫°ng s·ªë).\nNgo√†i ra, v√¨ m√¥ h√¨nh Transformer kh√¥ng c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu theo th·ª© t·ª± tu·∫ßn t·ª± (kh√°c v·ªõi RNN ho·∫∑c LSTM), n√≥ s·∫Ω c·∫ßn m·ªôt ch·ªâ b√°o ƒë·ªÉ ch·ªâ ra th·ª© t·ª± c·ªßa c√°c b∆∞·ªõc trong chu·ªói, g·ªçi l√† Postitional encoding. B·∫°n c√≥ th·ªÉ kham kh·∫£o b√†i vi·∫øt c·ªßa Mehreen Saeed. V√† code trong R s·∫Ω v√≠ d·ª± nh∆∞ sau:\n\n\nCode\npositional_encoding &lt;- function(seq_len, d, n = 10000) {\n  P &lt;- matrix(0, nrow = seq_len, ncol = d)\n  \n  for (k in 1:seq_len) {\n    for (i in 0:(d / 2 - 1)) {\n      denominator &lt;- n^(2 * i / d)\n      P[k, 2 * i + 1] &lt;- sin(k / denominator)\n      P[k, 2 * i + 2] &lt;- cos(k / denominator)\n    }\n  }\n  \n  return(P)\n}\n\n\n\nTh√†nh ph·∫ßn 3: Self-attention mechanism\n\nƒê√¢y l√† m·ªôt c∆° ch·∫ø ƒë·∫∑c bi·ªát c·ªßa Transformer, cho ph√©p m√¥ h√¨nh ch√∫ √Ω ƒë·∫øn t·∫•t c·∫£ c√°c b∆∞·ªõc th·ªùi gian tr∆∞·ªõc ƒë√≥ trong chu·ªói t·∫°i m·ªói b∆∞·ªõc. ƒêi·ªÅu n√†y gi√∫p m√¥ h√¨nh n·∫Øm b·∫Øt ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá d√†i h·∫°n v√† s·ª± li√™n h·ªá gi·ªØa c√°c b∆∞·ªõc th·ªùi gian v·ªõi nhau (gi√∫p tr√°nh g·∫∑p v·∫•n ƒë·ªÅ ghi nh·ªõ ng·∫Øn h·∫°n nh∆∞ RNN). B·∫°n c√≥ th·ªÉ xem Self-attention nh∆∞ l√† c·∫•u tr√∫c chung nh·∫•t, c√≤n khi x√¢y d·ª±ng m√¥ h√¨nh ng∆∞·ªùi ta c√≥ th·ªÉ bi·∫øn t·∫•u t√πy v√†o nhu c·∫ßu.\nNh∆∞ ·ªü Encoder th√¨ s·ª≠ d·ª•ng Multi-Head Attention c√≥ th·ªÉ t√≠nh to√°n ch√∫ √Ω nhi·ªÅu l·∫ßn song song (kh√°c v·ªõi self -attention ch·ªâ t√≠nh to√°n cho single sequence) . M·ªói ‚Äúƒë·∫ßu‚Äù c√≥ th·ªÉ ch√∫ √Ω ƒë·∫øn nh·ªØng kh√≠a c·∫°nh kh√°c nhau c·ªßa c√°c m·ªëi quan h·ªá th·ªùi gian trong chu·ªói. V√≠ d·ª•, m·ªôt ƒë·∫ßu c√≥ th·ªÉ ch√∫ √Ω ƒë·∫øn c√°c m·∫´u ng·∫Øn h·∫°n (v√≠ d·ª•: s·ª± dao ƒë·ªông h√†ng ng√†y), trong khi m·ªôt ƒë·∫ßu kh√°c c√≥ th·ªÉ n·∫Øm b·∫Øt c√°c xu h∆∞·ªõng d√†i h·∫°n (v√≠ d·ª•: chu k·ª≥ m√πa). Trong R th√¨ ƒë√£ c√≥ s·∫µn h√†m nn_multihead_attention() trong package torch.\nC√≤n ƒë·ªëi v·ªõi Decoder th√¨ d√πng Masked Multi-Head Attention ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng khi d·ª± b√°o gi√° tr·ªã ti·∫øp theo trong chu·ªói th·ªùi gian, m√¥ h√¨nh ch·ªâ c√≥ th·ªÉ ch√∫ √Ω ƒë·∫øn c√°c b∆∞·ªõc th·ªùi gian tr∆∞·ªõc ƒë√≥ m√† kh√¥ng nh√¨n v√†o c√°c b∆∞·ªõc th·ªùi gian t∆∞∆°ng lai. So s√°nh v·ªõi Self-attention th√¨ b·∫°n c·∫ßn th√™m b∆∞·ªõc Masked score th√¥i. Trong R s·∫Ω ƒë∆∞·ª£c code nh∆∞ sau:\n\n\n\n\n\n\nShow structure\nmask_self_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n\n    if (embed_dim %% num_heads != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    # Linear layers for Q, K, V \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n    # Final linear layer after concatenating heads\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n  },\n  \n  forward = function(x) {\n    batch_size &lt;- x$size(1)\n    seq_leng &lt;- x$size(2)\n    \n    # Linear projections for Q, K, V\n    Q &lt;- self$query(x)  # (batch_size, seq_leng, embed_dim)\n    K &lt;- self$key(x)\n    V &lt;- self$value(x)\n    \n    # Reshape to separate heads: (batch_size, num_heads, seq_leng, head_dim)\n    Q &lt;- Q$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    # Compute Matmul and scale:\n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    # Apply mask if provided\n    mask &lt;- torch_tril(torch_ones(c(seq_leng, seq_leng)))\n    \n    if (!is.null(mask)) {\n      \n      masked_attention_scores &lt;- attention_scores$masked_fill(mask == 0, -Inf)\n      \n    } else {\n      print(\"Warning: No mask provided\")\n    }\n    \n    # Compute attention weights\n    weights &lt;- nnf_softmax(masked_attention_scores, dim = -1)\n    \n    # Apply weights to V\n    attn_output &lt;- torch_matmul(weights, V)  \n    \n    # Reshape again:\n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng, self$embed_dim))\n    \n    # Final linear layer\n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n\n\n\nNgo√†i ra, trong Decoder c√≤n c√≥ Cross-attention nh∆∞ng n√≥ h∆°i ph·ª©c t·∫°p n√™n m√¨nh s·∫Ω gi·ªõi thi·ªáu sau.\n\nTh√†nh ph·∫ßn 4: Sub-layer\n\nB·∫°n s·∫Ω ƒë·ªÉ √Ω th·∫•y c√°c ph√©p t√≠nh to√°n trong m√¥ h√¨nh s·∫Ω lu√¥n k√®m theo b·ªô ph·∫≠n Add & Norm ƒë·ªÉ l∆∞u gi·ªØ residual v√† c·ªông v√†o output ƒë∆∞·ª£c t·∫°o sau khi k·∫øt th√∫c c√°c ph√©p t√≠nh ƒë√≥. Vi·ªác n√†y gi√∫p gi·∫£m thi·ªÉu v·∫•n ƒë·ªÅ vanishing gradient ƒë√£ ƒë·ªÅ c·∫≠p ·ªü trang tr∆∞·ªõc v√† gi√∫p cho m√¥ h√¨nh h·ªçc s√¢u h∆°n. Trong R b·∫°n ch·ªâ c·∫ßn th√™m l·ªõp n√†y b·∫±ng h√†m nn_layer_norm().\n\nTh√†nh ph·∫ßn 5: Feed-Forward Neural Networks\n\nSau khi t√≠nh to√°n ch√∫ √Ω, ƒë·∫°i di·ªán c·ªßa t·ª´ng b∆∞·ªõc th·ªùi gian s·∫Ω ƒë∆∞·ª£c ƒë∆∞a qua m·ªôt m·∫°ng n∆°-ron Feed-Forward (FFN), th∆∞·ªùng bao g·ªìm: (i) M·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh (l·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß), (ii) H√†m k√≠ch ho·∫°t ReLU v√† (iii) M·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh n·ªØa. Trong R s·∫Ω code nh∆∞ n√†y:\n\n\nCode\nfeed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )"
  },
  {
    "objectID": "transfer.html#c√°c-th√†nh-ph·∫ßn-ch√≠nh",
    "href": "transfer.html#c√°c-th√†nh-ph·∫ßn-ch√≠nh",
    "title": "M√¥ h√¨nh Transformer",
    "section": "3 C√°c th√†nh ph·∫ßn ch√≠nh:",
    "text": "3 C√°c th√†nh ph·∫ßn ch√≠nh:\nSau khi hi·ªÉu r√µ c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt, ta s·∫Ω ng√≥ qua workflow ƒë·∫ßy ƒë·ªß c·ªßa m√¥ h√¨nh Transformer.N·∫øu b·∫°n ch∆∞a hi·ªÉu th√¨ c√≥ th·ªÉ kham kh·∫£o link n√†y datacamp\n\n  \n  \n  \n  \n    H√¨nh 6: Workflow c·ªßa m√¥ h√¨nh Transformer\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nM√¨nh s·∫Ω tr√¨nh b√†y theo c√°ch c√° nh√¢n ƒë·ªÉ gi√∫p m·ªçi ng∆∞·ªùi hi·ªÉu r√µ h∆°n:\n\nB∆∞·ªõc 1: X·ª≠ l√≠ input: s·∫Ω g·ªìm b∆∞·ªõc Embedding d·ªØ li·ªáu sau ƒë√≥ c·ªông th√™m Positional encoding. L∆∞u √Ω: input cho encoder v√† decoder l√† kh√°c nhau, encoder s·∫Ω nh·∫≠n ƒë·∫ßu v√†o l√† c√°c bi·∫øn d·ª± b√°o (v√≠ d·ª•: gi√° tr·ªã lag c·ªßa time series,‚Ä¶) v√† decoder s·∫Ω nh·∫≠n ƒë·∫ßu v√†o l√† bi·∫øn target (l√† k·∫øt qu·∫£ b·∫°n mong mu·ªën m√¥ h√¨nh d·ª± b√°o ƒë√∫ng).\nB∆∞·ªõc 2: Encoder output: Khi d·ªØ li·ªáu ƒëi v√†o encoder block th√¨ s·∫Ω tr·∫£i qua l·ªõp multi-head attention* v√† feed forward v√† c√°c l·ªõp sub-layer normalization. L∆∞u √Ω: khi normalizing th√¨ ph·∫£i normalize (k·∫øt qu·∫£ t·ª´ l·ªõp tr∆∞·ªõc + input ban ƒë·∫ßu), b·∫°n c√≥ th·ªÉ nh√¨n ·∫£nh d∆∞·ªõi ƒë√¢y ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n.\n\n\n  \n  \n  \n  \n    H√¨nh 7: Normalization v√† residual connection sau l·ªõp Multi-Head Attention\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\nB∆∞·ªõc 3: Add encoder output to decoder: Sau khi Decoder th·ª±c hi·ªán t√≠nh to√°n cho d·ªØ li·ªáu th√¥ng qua layer Mask multi-head attention v√† Normalization th√¨ s·∫Ω ƒë·∫øn b∆∞·ªõc Cross-attention (M·∫∑c d√π ·ªü h√¨nh tr√™n ho·∫∑c c√°c t√†i li·ªáu kh√°c m√† b·∫°n t·ª´ng ƒë·ªçc s·∫Ω ƒë·ªÉ l√† layer multi-head attention nh∆∞ng th·ª±c ch·∫•t layer cross-attention m·ªõi ƒë√∫ng).\n\nV·∫≠y cross-attention c√≥ g√¨ ƒë·∫∑c bi·ªát? Ta s·∫Ω nh√¨n s∆° qua c·∫•u tr√∫c c·ªßa n√≥ th√¨ s·∫Ω nh·∫≠n ra ƒëi·ªÉm kh√°c bi·ªát so v·ªõi self-attention th√¥ng th∆∞·ªùng l√† cross-attention s·∫Ω nh·∫≠n d·ªØ li·ªáu t·ª´ 2 ngu·ªìn: (i) output c·ªßa encoder g√°n cho Q v√† (ii) input c·ªßa decoder g√°n cho V, K.\n\nCross attention:Self attention:\n\n\n\n  \n  \n  \n  \n    H√¨nh 8: Workflow c·ªßa Cross-attention\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\n\n\n  \n  \n  \n  \n    H√¨nh 9: Workflow c·ªßa Self-attention\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\n\n\nV·ªÅ code trong R s·∫Ω nh∆∞ sau:\n\n\nShow structure\ncross_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n  },\n  \n  forward = function(decoder_input, encoder_output, mask = NULL) {\n    batch_size &lt;- decoder_input$size(1)\n    seq_leng_dec &lt;- decoder_input$size(2)\n    seq_leng_enc &lt;- encoder_output$size(2)\n    \n    Q &lt;- self$query(decoder_input)\n    K &lt;- self$key(encoder_output)\n    V &lt;- self$value(encoder_output)\n    \n    Q &lt;- Q$view(c(batch_size, seq_leng_dec, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    weights &lt;- nnf_softmax(attention_scores, dim = -1)\n    \n    attn_output &lt;- torch_matmul(weights, V)\n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng_dec, self$embed_dim))\n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n\nK·∫øt qu·∫£ sau ƒë√≥ s·∫Ω ƒë∆∞·ª£c ƒë·∫©y qua layer feed forward v√† normalization ƒë·ªÉ tr·∫£ v·ªÅ output (gi·ªëng nh∆∞ encoder).\n\nB∆∞·ªõc 4: Output of decoder: Cu·ªëi c√πng, output c·ªßa decoder s·∫Ω qua 2 layer linear v√† softmax ƒë·ªÉ t√¨m ra output c√≥ x√°c su·∫•t cao nh·∫•t (nghƒ©a l√† output ƒë√≥ s·∫Ω c√≥ √Ω nghƒ©a nh·∫•t trong sequence ƒë·ªÉ d·ª± b√°o cho c√°c step sau).\n\n\n  \n  \n  \n  \n    H√¨nh 10: Output c·ªßa m√¥ h√¨nh\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nNh∆∞ v·∫≠y, ch√∫ng ta ƒë√£ l∆∞·ªõt s∆° qua c√°ch ho·∫°t ƒë·ªông v√† c√°c l∆∞u √Ω c·ªßa m√¥ h√¨nh Transformer. Ti·∫øp theo, m√¨nh s·∫Ω th·ª≠ x√¢y d·ª±ng trong R v√† d√πng n√≥ ƒë·ªÉ x·ª≠ l√≠ task d·ª± b√°o chu·ªói th·ªùi gian.\n\n\n\n    \n    \n    Go to Next Page\n    \n\n\n    \n        \n            Go to Next Page\n            ‚ûî"
  }
]